1115201400190 Προκόπιος Σταμελιάς
1115201600095 Άννα Λογοθέτη

link Git:

Τίτλος Προγράμματος: Μέτρηση ομοιότητας εικόνων με Manhattan (με εξαντλητική αναζήτηση ή με LSH) και EMD, στον αρχικό
                    και στον νέο διανυσματικό χώρο των εικόνων. Συσταδοποίηση στον αρχικό και στον νέο χώρο. Συσταδοποίηση
                    με την χρήση του κατηγοριοποιητή που φτιάχτηκε στην 2η εργασία.
Οδηγίες χρήσης:
    Εκτέλεση Α ερωτήματος: python3 reduce.py -d <dataset> -q <queryset> -od <output_dataset_file> -oq <output_query_file>
        (Μέσα στα αρχεία έχουμε παραδώσει και τα output_dataset_file και output_query_file που είναι τα outData και outQuery
        αντίστοιχα. Επίσης έχουμε παραδώσει και ένα προεκπαιδευμένο μοντέλο αυτοκωδικοποιητή, το autoencoder.h5, γιατί το
        Α ερώτημα όταν αρχίσει να τρέχει ρωτάει τον χρήστη αν θέλει να χρησιμοποιήσει έναν προεκπαιδευμένο αυτοκωδικοποιητή
        ή να δημιουργήσει νέα μοντέλα και να τα εκπαιδεύσει.)
    Εκτέλεση Β ερωτήματος: make (πρέπει πρώτα να μεταγλωττίσει ο χρήστης)
                           ./search -d <input file original space> -i <input file new space> –q <query file original space> -s <query file new space> –k <int> -L <int> -ο <output file>
        (Για <input file new space> χρησιμοποιήσαμε το outData που πήραμε από το Α ερώτημα και για <query file new space>
        χρησιμοποιήσαμε το outQuery. Ακόμα για k βάλαμε το 4 και για L το 3. Το αποτέλεσμα του Β ερωτήματος αποθηκεύτηκε
        στο outofB, δηλαδή το <output file> το ονομάσαμε outofB και το παραδώσαμε. Επίσης, για τις ανάγκες του Γ ερωτήματος,
        όπου χρειάζεται να δώσουμε το average correct της Manhattan με εξαντλητική αναζήτηση η οποία εκτελείται στο Β ερώτημα,
        όταν αρχίζει και τρέχει το πρόγραμμα του Β ερωτήματος, ζητάει από τον χρήστη να του δώσει το αρχείο με τα labels του
        input file, το αρχείο με τα labels του query file και το αρχείο εξόδου στο οποίο αποθήκευσε το Γ ερώτημα τα αποτελέσματα του,
        στην προκειμένη περίπτωση το αρχείο αυτό είναι το outputC_7)
    Εκτέλεση Γ ερωτήματος: python3 search.py –d <input file original space> –q <query file original space> -l1 <labels of input dataset> -l2 <labels of query dataset> -ο <output file> -EMD
    Εκτέλεση Δ ερωτήματος: make (πρέπει πρώτα να μεταγλωττίσει ο χρήστης)
                          ./cluster –d <input file original space> -i <input file new space> -n <classes from NN as clusters file> –c <configuration file> -o <output file>
        (Για <configuration file> δίνεται το cluster.conf)


Κατάλογος αρχείων κώδικα και περιγραφή τους:
     reduce.py: Αρχείο κώδικα που εκτελείται για τις ανάγκες του Α ερωτήματος. Ο αυτοκωδικοποιητής έχει εμπλουτιστεί
              με κάποια επιπλέον επίπεδα ούτως ώστε να μετατρέπει κάθε εικόνα από τις αρχικές τις διαστάσεις σε νέες διαστάσεις(latent dimension).
              Ο κώδικας αυτός πέρα από την παραπάνω δυνατότητα που δίνει στον χρήστη, δίνει και κάποιες ακόμα επιλογές:1)ο χρήστης μπορεί
              να χρησιμοποιήσεις κάποιο προεκπαιδευμένο μοντέλο αυτοκωδικοποιητή ώστε να μετατρέψει ένα αρχείο με εικόνες από τον παλιό
              σε κάποιον νέο διανυσματικό χώρο,2) ο χρήστης μετά το τελός της εκπαίδευσης ενός μοντέλου που μόλις έφτιαξε, μπορεί να επαναλάβει
              την διαδικασία για την δημιουργία ενός άλλου μοντέλου,3)ο χρήστης μπορεί να φτιάξει γραφικές παραστάσεις του σφάλματος συναρτήσει
              των τιμών των υπερπαραμέτρων που έδωσε στα πειράματα που εκτέλεσε μέχρι στιγμής,4)ο χρήστης μπορεί να σώσει το μοντέλο που μόλις
              εκπαίδευσε.
      mainB.c,funB.c,structB.h: Αρχεία κώδικα που εκτελούνται για τις ανάγκες του Β ερωτήματος. Σκοπός του συγκεκριμένου πηγαίου κώδικα
              είναι για κάθε εικόνα του <query file original space> να βρεθεί ο προσεγγιστικά πλησιέστερος γείτονας κατά LSH και ο ακριβής πλησιέστερος
              γείτονας στο <input file original space>, και για κάθε εικόνα στο <query file new space> να βρεθεί με εξαντλητική αναζήτηση ο πλησιέστερος
              γείτονας της στο <input file new space>. Στο τέλος εκτυπώνονται επίσης οι συνολικοί χρόνοι αναζήτησης για τον LSH και την εξαντλητική
              αναζήτηση στον αρχικό χώρο, ο συνολικός χρόνος εξαντλητικής αναζήτησης στον νέο χώρο και τα κλάσματα προσέγγισης για τον LSH και την
              εξαντλητική αναζήτηση στον νέο χώρο.
      search.py: Αρχείο κώδικα που εκτελείται για τις ανάγκες του Γ ερωτήματος. Ο κώδικας αυτός βρίσκει για κάθε εικόνα του <query file original space>
              τους 10 πλησιέστερους της γείτονες στο αρχείο <input file original space> με την χρήση της EMD μετρικής. Για τις ανάγκες της EMD μετρικής
              χρησιμοποιήθηκε η συνάρτηση γραμμικού προγραμματισμού της βιβλιοθήκης scipy, η linprog. Ακόμα με την χρήση των labels αρχείων του input file
              και του query file υπολογίζεται το average correct της EMD δηλαδή η ορθότητα της, καθώς επίσης εκτυπώνεται και ο χρόνος εκτέλεσης της EMD
              μετρικής.
      mainD.c,funD.c,structD.h: Αρχεία κώδικα που εκτελούνται για τις ανάγκες του Δ ερωτήματος. Εκτελείται συσταδοποίηση στον αρχικό διανυσματικό
              χώρο και στον νέο διανυσματικό χώρο, καθώς επίσης υπολογίζονται τα silhouettes και η τίμη της objective function σε κάθε έναν από
              τους δύο χώρους. Τα αποτελέσματα σώζονται στο <output file>.


Αποτελέσματα πειραμάτων:

      Πειράματα για το Α ερώτημα:
              (Σημείωση: Για να καταλάβουμε πώς επηρεάζει το σφάλμα μία υπερπαράμετρος, πρέπει να εκτελέσουμε διαδοχικά πειράματα στα οποία δίνουμε
              διαφορετικές τιμές μόνο στην υπερπαράμετρο που μας ενδιαφέρει, και τις υπόλοιπες υπερπαραμέτρους τις αφήνουμε σταθερές.)
              autoencoderEpochs.png: Καθώς αυξάνουμε το πλήθος των εποχών, μειώνεται το σφάλμα.
              autoencoderBatchSize.png: Καθώς αυξάνουμε το batch size, αυξάνεται το σφάλμα.
              autoencoderLatentDim: (δεν εκτυπώνεται γραφική παράσταση για το latent dimension αλλά εκτελέστηκαν πειράματα στα οποία δόθηκαν στο
                                  latent dimension οι τιμές 5,10 και 20, έχοντας τις υπόλοιπες υπερπαραμέτρους σταθερές)
                                  Καθώς αυξάνεται το latent dimension, μειώνεται το σφάλμα.
              autoencoderFilters.png: Καθώς αυξάνεται το πλήθος των φίλτρων σε ένα συνελικτικό στρώμα, μειώνεται το σφάλμα.
      Πειράματα για το Β ερώτημα:
              outofB: Παρατηρούμε ότι ο χρόνος εύρεσης πλησιέστερου γείτονα στον νέο διανυσματικό χώρο(tReduced) είναι αρκετά μικρότερος από τον χρόνο
                      εύρεσης πλησιέστερου γείτονα με LSH (tLSH) στον αρχικό διανυσματικό χώρο, ο οποίος είναι αρκετά μικρότερος από τον χρόνο εύρεσης
                      πλησιέστερου γείτονα στον αρχικό διανυσματικό χώρο με εξαντλητική αναζήτηση(tTrue). Επίσης παρατηρούμε ότι το κλάσμα προσέγγισης
                      στον LSH είναι μικρότερο από το κλάσμα προσέγγισης στον νεό διανυσματικό χώρο (πράγμα που δηλώνει ότι η εξαντλητική αναζήτηση στον
                      νέο διανυσματικό χώρο λειτούργησε κατά ελάχιστο ορθότερα από την LSH μέθοδο).
      Πειράματα για το Γ ερώτημα:
              outputC_4,timeC_4: Ορθότητα EMD και χρόνος για cluster_size = 4
              outputC_7,timeC_7: Ορθότητα EMD και χρόνος για cluster_size = 7
              outputC_14,timeC_14: Ορθότητα EMD και χρόνος για cluster_size = 14
              Παρατηρούμε ότι:1) όσο αυξάνεται το μέγεθος του cluster, τόσο μειώνεται ο χρόνος εκτέλεσης.
                              2) όσο αυξάνεται το μέγεθος του cluster, τόσο μειώνεται η ορθότητα.
              Ακόμα, συγκριτικά με το average correct της εξαντλητικής αναζήτησης του Β ερωτήματος παρατηρούμε ότι
              η εξαντλητική αναζήτηση του Β ερωτήματος στον αρχικό διανυσματικό χώρο έδωσε πιο ορθά αποτελέσματα από την
              EMD με cluster size = 7 (βλέπε outputC_7).
      Πειράματα για το Δ ερώτημα:
              outofD: Παρατηρούμε ότι η συσταδοποίηση στον νέο διανυσματικό χώρο γίνεται καλύτερα από την συσταδοποίηση στον αρχικό διανυσματικό χώρο
                     καθώς, διαμοιράζονται στα κέντρα πιο ομοιόμορφα οι εικόνες, δίνονται καλύτερα silhouettes,το clustering παίρνει λιγότερο χρόνο
                     και το objective function έχει μικρότερη τιμή.
